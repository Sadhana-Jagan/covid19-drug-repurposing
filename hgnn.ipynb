{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.sparse import coo_matrix, csr_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# === HGNN Layer ===\n",
        "class HGNNConv(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(HGNNConv, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features)\n",
        "\n",
        "    def forward(self, x, G):\n",
        "        x = self.linear(x)\n",
        "        x = torch.spmm(G, x)  # Sparse matrix multiplication\n",
        "        return F.relu(x)\n",
        "\n",
        "# === HGNN Model ===\n",
        "class HGNN(nn.Module):\n",
        "    def __init__(self, in_features, hidden_dim, out_features):\n",
        "        super(HGNN, self).__init__()\n",
        "        self.layer1 = HGNNConv(in_features, hidden_dim)\n",
        "        self.layer2 = HGNNConv(hidden_dim, out_features)\n",
        "\n",
        "    def forward(self, x, G):\n",
        "        x = self.layer1(x, G)\n",
        "        x = self.layer2(x, G)\n",
        "        return x\n",
        "\n",
        "# === Build Hypergraph Incidence Matrix ===\n",
        "def build_incidence_matrix(num_nodes, edge_index):\n",
        "    rows, cols = [], []\n",
        "    for i, (u, v) in enumerate(edge_index):\n",
        "        rows += [u, v]\n",
        "        cols += [i, i]\n",
        "    data = np.ones(len(rows))\n",
        "    H = coo_matrix((data, (rows, cols)), shape=(num_nodes, len(edge_index))).tocoo()\n",
        "    return H\n",
        "\n",
        "def build_incidence_matrix_from_adjacency(adj_matrix):\n",
        "    # Ensure symmetry\n",
        "    adj_matrix = np.maximum(adj_matrix, adj_matrix.T)\n",
        "\n",
        "    n = adj_matrix.shape[0]\n",
        "    edge_index = []\n",
        "\n",
        "    # Add self-loops for isolated nodes\n",
        "    isolated_nodes = []\n",
        "    for i in range(n):\n",
        "        connected = False\n",
        "        for j in range(n):\n",
        "            if i != j and adj_matrix[i, j] != 0:\n",
        "                edge_index.append([i, j])\n",
        "                connected = True\n",
        "        if not connected:\n",
        "            isolated_nodes.append(i)\n",
        "\n",
        "    # Add self-loop to isolated nodes\n",
        "    for node in isolated_nodes:\n",
        "        edge_index.append([node, node])\n",
        "\n",
        "    print(f\"Added self-loops for {len(isolated_nodes)} isolated nodes.\")\n",
        "    return build_incidence_matrix(n, edge_index)\n",
        "\n",
        "def normalize_incidence_matrix(H):\n",
        "    H = H.astype(np.float32)\n",
        "    Dv = np.array(H.sum(1)).flatten()\n",
        "    De = np.array(H.sum(0)).flatten()\n",
        "\n",
        "    Dv_inv_sqrt = 1.0 / np.sqrt(Dv + 1e-6)\n",
        "    De_inv = 1.0 / (De + 1e-6)\n",
        "\n",
        "    Dv_inv_sqrt_mat = coo_matrix((Dv_inv_sqrt, (np.arange(len(Dv)), np.arange(len(Dv)))), shape=(len(Dv), len(Dv)))\n",
        "    De_inv_mat = coo_matrix((De_inv, (np.arange(len(De)), np.arange(len(De)))), shape=(len(De), len(De)))\n",
        "\n",
        "    HT = H.transpose()\n",
        "    G = Dv_inv_sqrt_mat @ H @ De_inv_mat @ HT @ Dv_inv_sqrt_mat\n",
        "    return torch.FloatTensor(G.todense())\n",
        "\n",
        "# === Load Preprocessed Data ===\n",
        "def load_data(features_file, adjacency_file):\n",
        "    features_df = pd.read_csv(features_file)\n",
        "    drug_names = features_df.iloc[:, 0].values\n",
        "    raw_features = features_df.iloc[:, 1:].values\n",
        "\n",
        "    # Feature normalization\n",
        "    scaler = StandardScaler()\n",
        "    features = scaler.fit_transform(raw_features)\n",
        "    features = torch.FloatTensor(features)\n",
        "\n",
        "    adj_df = pd.read_csv(adjacency_file)\n",
        "    adj_matrix = adj_df.iloc[:, 1:].values.astype(np.float32)\n",
        "\n",
        "    return features, adj_matrix, drug_names\n",
        "\n",
        "# === Training Function ===\n",
        "def train_hgnn(features_csv='/content/all_drug_data_processed.csv',\n",
        "               adjacency_csv='/content/DDR-HC-new.csv',\n",
        "               hidden_dim=32, embedding_dim=16, epochs=5000):\n",
        "    print(\"Loading data...\")\n",
        "    X, adj_matrix, drug_names = load_data(features_csv, adjacency_csv)\n",
        "\n",
        "    print(\"Building hypergraph...\")\n",
        "    H = build_incidence_matrix_from_adjacency(adj_matrix)\n",
        "    G = normalize_incidence_matrix(H)\n",
        "\n",
        "    print(\"Initializing HGNN...\")\n",
        "    model = HGNN(X.shape[1], hidden_dim, embedding_dim)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    print(\"Training HGNN...\")\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(X, G)\n",
        "\n",
        "        # MSE loss between 16-dim output and input\n",
        "        loss = F.mse_loss(out, X[:, :embedding_dim])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    embeddings = out.detach().numpy()\n",
        "\n",
        "    # Save embeddings with drug names\n",
        "    df_embed = pd.DataFrame(embeddings, columns=[f'embedding_{i}' for i in range(embedding_dim)])\n",
        "    df_embed.insert(0, 'drug_name', drug_names)\n",
        "    df_embed.to_csv(\"drug_embeddings.csv\", index=False)\n",
        "    print(\"Embeddings saved to drug_embeddings.csv\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_hgnn()\n"
      ],
      "metadata": {
        "id": "cDfncCenSd7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7f20dfd-9396-4fa2-c107-31fa80dce5e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Building hypergraph...\n",
            "Added self-loops for 67 isolated nodes.\n",
            "Initializing HGNN...\n",
            "Training HGNN...\n",
            "Epoch 0, Loss: 1.0053\n",
            "Epoch 20, Loss: 0.9483\n",
            "Epoch 40, Loss: 0.8916\n",
            "Epoch 60, Loss: 0.8290\n",
            "Epoch 80, Loss: 0.7604\n",
            "Epoch 100, Loss: 0.6925\n",
            "Epoch 120, Loss: 0.6298\n",
            "Epoch 140, Loss: 0.5675\n",
            "Epoch 160, Loss: 0.5081\n",
            "Epoch 180, Loss: 0.4508\n",
            "Epoch 200, Loss: 0.4061\n",
            "Epoch 220, Loss: 0.3716\n",
            "Epoch 240, Loss: 0.3454\n",
            "Epoch 260, Loss: 0.3253\n",
            "Epoch 280, Loss: 0.3101\n",
            "Epoch 300, Loss: 0.2983\n",
            "Epoch 320, Loss: 0.2888\n",
            "Epoch 340, Loss: 0.2808\n",
            "Epoch 360, Loss: 0.2739\n",
            "Epoch 380, Loss: 0.2679\n",
            "Epoch 400, Loss: 0.2628\n",
            "Epoch 420, Loss: 0.2583\n",
            "Epoch 440, Loss: 0.2540\n",
            "Epoch 460, Loss: 0.2503\n",
            "Epoch 480, Loss: 0.2470\n",
            "Epoch 500, Loss: 0.2441\n",
            "Epoch 520, Loss: 0.2416\n",
            "Epoch 540, Loss: 0.2394\n",
            "Epoch 560, Loss: 0.2373\n",
            "Epoch 580, Loss: 0.2355\n",
            "Epoch 600, Loss: 0.2339\n",
            "Epoch 620, Loss: 0.2324\n",
            "Epoch 640, Loss: 0.2310\n",
            "Epoch 660, Loss: 0.2297\n",
            "Epoch 680, Loss: 0.2285\n",
            "Epoch 700, Loss: 0.2273\n",
            "Epoch 720, Loss: 0.2262\n",
            "Epoch 740, Loss: 0.2252\n",
            "Epoch 760, Loss: 0.2243\n",
            "Epoch 780, Loss: 0.2233\n",
            "Epoch 800, Loss: 0.2224\n",
            "Epoch 820, Loss: 0.2216\n",
            "Epoch 840, Loss: 0.2208\n",
            "Epoch 860, Loss: 0.2200\n",
            "Epoch 880, Loss: 0.2192\n",
            "Epoch 900, Loss: 0.2185\n",
            "Epoch 920, Loss: 0.2178\n",
            "Epoch 940, Loss: 0.2172\n",
            "Epoch 960, Loss: 0.2165\n",
            "Epoch 980, Loss: 0.2159\n",
            "Epoch 1000, Loss: 0.2152\n",
            "Epoch 1020, Loss: 0.2146\n",
            "Epoch 1040, Loss: 0.2140\n",
            "Epoch 1060, Loss: 0.2134\n",
            "Epoch 1080, Loss: 0.2129\n",
            "Epoch 1100, Loss: 0.2123\n",
            "Epoch 1120, Loss: 0.2119\n",
            "Epoch 1140, Loss: 0.2114\n",
            "Epoch 1160, Loss: 0.2109\n",
            "Epoch 1180, Loss: 0.2105\n",
            "Epoch 1200, Loss: 0.2100\n",
            "Epoch 1220, Loss: 0.2096\n",
            "Epoch 1240, Loss: 0.2092\n",
            "Epoch 1260, Loss: 0.2087\n",
            "Epoch 1280, Loss: 0.2083\n",
            "Epoch 1300, Loss: 0.2079\n",
            "Epoch 1320, Loss: 0.2075\n",
            "Epoch 1340, Loss: 0.2072\n",
            "Epoch 1360, Loss: 0.2068\n",
            "Epoch 1380, Loss: 0.2064\n",
            "Epoch 1400, Loss: 0.2058\n",
            "Epoch 1420, Loss: 0.2054\n",
            "Epoch 1440, Loss: 0.2050\n",
            "Epoch 1460, Loss: 0.2047\n",
            "Epoch 1480, Loss: 0.2043\n",
            "Epoch 1500, Loss: 0.2040\n",
            "Epoch 1520, Loss: 0.2037\n",
            "Epoch 1540, Loss: 0.2034\n",
            "Epoch 1560, Loss: 0.2032\n",
            "Epoch 1580, Loss: 0.2029\n",
            "Epoch 1600, Loss: 0.2026\n",
            "Epoch 1620, Loss: 0.2024\n",
            "Epoch 1640, Loss: 0.2021\n",
            "Epoch 1660, Loss: 0.2019\n",
            "Epoch 1680, Loss: 0.2017\n",
            "Epoch 1700, Loss: 0.2014\n",
            "Epoch 1720, Loss: 0.2012\n",
            "Epoch 1740, Loss: 0.2010\n",
            "Epoch 1760, Loss: 0.2008\n",
            "Epoch 1780, Loss: 0.2006\n",
            "Epoch 1800, Loss: 0.2004\n",
            "Epoch 1820, Loss: 0.2001\n",
            "Epoch 1840, Loss: 0.1999\n",
            "Epoch 1860, Loss: 0.1997\n",
            "Epoch 1880, Loss: 0.1995\n",
            "Epoch 1900, Loss: 0.1993\n",
            "Epoch 1920, Loss: 0.1991\n",
            "Epoch 1940, Loss: 0.1989\n",
            "Epoch 1960, Loss: 0.1988\n",
            "Epoch 1980, Loss: 0.1986\n",
            "Epoch 2000, Loss: 0.1984\n",
            "Epoch 2020, Loss: 0.1983\n",
            "Epoch 2040, Loss: 0.1981\n",
            "Epoch 2060, Loss: 0.1980\n",
            "Epoch 2080, Loss: 0.1978\n",
            "Epoch 2100, Loss: 0.1977\n",
            "Epoch 2120, Loss: 0.1975\n",
            "Epoch 2140, Loss: 0.1974\n",
            "Epoch 2160, Loss: 0.1972\n",
            "Epoch 2180, Loss: 0.1971\n",
            "Epoch 2200, Loss: 0.1970\n",
            "Epoch 2220, Loss: 0.1968\n",
            "Epoch 2240, Loss: 0.1967\n",
            "Epoch 2260, Loss: 0.1965\n",
            "Epoch 2280, Loss: 0.1964\n",
            "Epoch 2300, Loss: 0.1962\n",
            "Epoch 2320, Loss: 0.1961\n",
            "Epoch 2340, Loss: 0.1960\n",
            "Epoch 2360, Loss: 0.1959\n",
            "Epoch 2380, Loss: 0.1957\n",
            "Epoch 2400, Loss: 0.1956\n",
            "Epoch 2420, Loss: 0.1955\n",
            "Epoch 2440, Loss: 0.1954\n",
            "Epoch 2460, Loss: 0.1953\n",
            "Epoch 2480, Loss: 0.1952\n",
            "Epoch 2500, Loss: 0.1951\n",
            "Epoch 2520, Loss: 0.1950\n",
            "Epoch 2540, Loss: 0.1949\n",
            "Epoch 2560, Loss: 0.1948\n",
            "Epoch 2580, Loss: 0.1947\n",
            "Epoch 2600, Loss: 0.1946\n",
            "Epoch 2620, Loss: 0.1945\n",
            "Epoch 2640, Loss: 0.1944\n",
            "Epoch 2660, Loss: 0.1943\n",
            "Epoch 2680, Loss: 0.1942\n",
            "Epoch 2700, Loss: 0.1941\n",
            "Epoch 2720, Loss: 0.1940\n",
            "Epoch 2740, Loss: 0.1940\n",
            "Epoch 2760, Loss: 0.1939\n",
            "Epoch 2780, Loss: 0.1938\n",
            "Epoch 2800, Loss: 0.1937\n",
            "Epoch 2820, Loss: 0.1936\n",
            "Epoch 2840, Loss: 0.1935\n",
            "Epoch 2860, Loss: 0.1935\n",
            "Epoch 2880, Loss: 0.1934\n",
            "Epoch 2900, Loss: 0.1933\n",
            "Epoch 2920, Loss: 0.1933\n",
            "Epoch 2940, Loss: 0.1932\n",
            "Epoch 2960, Loss: 0.1931\n",
            "Epoch 2980, Loss: 0.1930\n",
            "Epoch 3000, Loss: 0.1929\n",
            "Epoch 3020, Loss: 0.1929\n",
            "Epoch 3040, Loss: 0.1928\n",
            "Epoch 3060, Loss: 0.1927\n",
            "Epoch 3080, Loss: 0.1927\n",
            "Epoch 3100, Loss: 0.1926\n",
            "Epoch 3120, Loss: 0.1925\n",
            "Epoch 3140, Loss: 0.1925\n",
            "Epoch 3160, Loss: 0.1924\n",
            "Epoch 3180, Loss: 0.1923\n",
            "Epoch 3200, Loss: 0.1923\n",
            "Epoch 3220, Loss: 0.1922\n",
            "Epoch 3240, Loss: 0.1922\n",
            "Epoch 3260, Loss: 0.1921\n",
            "Epoch 3280, Loss: 0.1921\n",
            "Epoch 3300, Loss: 0.1920\n",
            "Epoch 3320, Loss: 0.1919\n",
            "Epoch 3340, Loss: 0.1919\n",
            "Epoch 3360, Loss: 0.1918\n",
            "Epoch 3380, Loss: 0.1918\n",
            "Epoch 3400, Loss: 0.1917\n",
            "Epoch 3420, Loss: 0.1917\n",
            "Epoch 3440, Loss: 0.1916\n",
            "Epoch 3460, Loss: 0.1915\n",
            "Epoch 3480, Loss: 0.1915\n",
            "Epoch 3500, Loss: 0.1914\n",
            "Epoch 3520, Loss: 0.1914\n",
            "Epoch 3540, Loss: 0.1914\n",
            "Epoch 3560, Loss: 0.1913\n",
            "Epoch 3580, Loss: 0.1913\n",
            "Epoch 3600, Loss: 0.1912\n",
            "Epoch 3620, Loss: 0.1912\n",
            "Epoch 3640, Loss: 0.1911\n",
            "Epoch 3660, Loss: 0.1911\n",
            "Epoch 3680, Loss: 0.1910\n",
            "Epoch 3700, Loss: 0.1910\n",
            "Epoch 3720, Loss: 0.1910\n",
            "Epoch 3740, Loss: 0.1909\n",
            "Epoch 3760, Loss: 0.1909\n",
            "Epoch 3780, Loss: 0.1908\n",
            "Epoch 3800, Loss: 0.1908\n",
            "Epoch 3820, Loss: 0.1908\n",
            "Epoch 3840, Loss: 0.1907\n",
            "Epoch 3860, Loss: 0.1907\n",
            "Epoch 3880, Loss: 0.1906\n",
            "Epoch 3900, Loss: 0.1906\n",
            "Epoch 3920, Loss: 0.1906\n",
            "Epoch 3940, Loss: 0.1905\n",
            "Epoch 3960, Loss: 0.1905\n",
            "Epoch 3980, Loss: 0.1905\n",
            "Epoch 4000, Loss: 0.1904\n",
            "Epoch 4020, Loss: 0.1904\n",
            "Epoch 4040, Loss: 0.1903\n",
            "Epoch 4060, Loss: 0.1903\n",
            "Epoch 4080, Loss: 0.1903\n",
            "Epoch 4100, Loss: 0.1902\n",
            "Epoch 4120, Loss: 0.1902\n",
            "Epoch 4140, Loss: 0.1902\n",
            "Epoch 4160, Loss: 0.1901\n",
            "Epoch 4180, Loss: 0.1901\n",
            "Epoch 4200, Loss: 0.1900\n",
            "Epoch 4220, Loss: 0.1900\n",
            "Epoch 4240, Loss: 0.1900\n",
            "Epoch 4260, Loss: 0.1900\n",
            "Epoch 4280, Loss: 0.1899\n",
            "Epoch 4300, Loss: 0.1899\n",
            "Epoch 4320, Loss: 0.1898\n",
            "Epoch 4340, Loss: 0.1898\n",
            "Epoch 4360, Loss: 0.1897\n",
            "Epoch 4380, Loss: 0.1897\n",
            "Epoch 4400, Loss: 0.1897\n",
            "Epoch 4420, Loss: 0.1896\n",
            "Epoch 4440, Loss: 0.1896\n",
            "Epoch 4460, Loss: 0.1896\n",
            "Epoch 4480, Loss: 0.1895\n",
            "Epoch 4500, Loss: 0.1895\n",
            "Epoch 4520, Loss: 0.1895\n",
            "Epoch 4540, Loss: 0.1894\n",
            "Epoch 4560, Loss: 0.1894\n",
            "Epoch 4580, Loss: 0.1894\n",
            "Epoch 4600, Loss: 0.1893\n",
            "Epoch 4620, Loss: 0.1893\n",
            "Epoch 4640, Loss: 0.1893\n",
            "Epoch 4660, Loss: 0.1893\n",
            "Epoch 4680, Loss: 0.1892\n",
            "Epoch 4700, Loss: 0.1892\n",
            "Epoch 4720, Loss: 0.1892\n",
            "Epoch 4740, Loss: 0.1891\n",
            "Epoch 4760, Loss: 0.1891\n",
            "Epoch 4780, Loss: 0.1891\n",
            "Epoch 4800, Loss: 0.1890\n",
            "Epoch 4820, Loss: 0.1890\n",
            "Epoch 4840, Loss: 0.1890\n",
            "Epoch 4860, Loss: 0.1889\n",
            "Epoch 4880, Loss: 0.1889\n",
            "Epoch 4900, Loss: 0.1889\n",
            "Epoch 4920, Loss: 0.1889\n",
            "Epoch 4940, Loss: 0.1888\n",
            "Epoch 4960, Loss: 0.1888\n",
            "Epoch 4980, Loss: 0.1888\n",
            "Embeddings saved to drug_embeddings.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kki91QzoXAUr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}