{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11037106,"sourceType":"datasetVersion","datasetId":6874571},{"sourceId":11037141,"sourceType":"datasetVersion","datasetId":6874598},{"sourceId":11037185,"sourceType":"datasetVersion","datasetId":6874632}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Forces floating-point division (/) instead of integer division.\nfrom __future__ import division\nfrom __future__ import print_function\nimport matplotlib.pyplot as plt\n#Uses TensorFlow v1 behavior in TF v2.\nimport tensorflow.compat.v1 as tf\nimport numpy as np\nimport scipy.sparse as sp\n#Standardizes dataset (zero mean, unit variance).\nfrom sklearn.preprocessing import StandardScaler\nimport time\nimport pandas as pd\nimport os\n#Computes AUC-ROC (classification metric).\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import average_precision_score\nimport warnings\nimport keras\n\nwarnings.filterwarnings('ignore')\ntf.compat.v1.disable_eager_execution()","metadata":{"id":"42Qkzr6ziZAS","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:15:18.500838Z","iopub.execute_input":"2025-03-15T17:15:18.501229Z","iopub.status.idle":"2025-03-15T17:15:18.506367Z","shell.execute_reply.started":"2025-03-15T17:15:18.501194Z","shell.execute_reply":"2025-03-15T17:15:18.505530Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n    \"\"\"Create a weight variable with Glorot & Bengio (AISTATS 2010)\n    initialization.\n    \"\"\"\n    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n    initial = tf.random_uniform([input_dim, output_dim], minval=-init_range,\n                                maxval=init_range, dtype=tf.float32)\n    return tf.Variable(initial, name=name)","metadata":{"id":"D6d6lZYVdTJR","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:15:18.507615Z","iopub.execute_input":"2025-03-15T17:15:18.507931Z","iopub.status.idle":"2025-03-15T17:15:18.518282Z","shell.execute_reply.started":"2025-03-15T17:15:18.507900Z","shell.execute_reply":"2025-03-15T17:15:18.517490Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# global unique layer ID dictionary for layer name assignment\n_LAYER_UIDS = {}\ndef get_layer_uid(layer_name=''):\n    \"\"\"Helper function, assigns unique layer IDs\n    \"\"\"\n    if layer_name not in _LAYER_UIDS:\n        _LAYER_UIDS[layer_name] = 1\n        return 1\n    else:\n        _LAYER_UIDS[layer_name] += 1\n        return _LAYER_UIDS[layer_name]\ndef dropout_sparse(x, keep_prob, num_nonzero_elems):\n    \"\"\"Dropout for sparse tensors. Currently fails for very large sparse tensors (>1M elements)\n    \"\"\"\n    noise_shape = [num_nonzero_elems]\n    random_tensor = keep_prob\n    random_tensor += tf.random_uniform(noise_shape)\n    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n    pre_out = tf.sparse_retain(x, dropout_mask)\n    return pre_out * (1./keep_prob)\nclass Layer(object):\n    \"\"\"Base layer class. Defines basic API for all layer objects.\n    # Properties\n        name: String, defines the variable scope of the layer.\n    # Methods\n        _call(inputs): Defines computation graph of layer\n            (i.e. takes input, returns output)\n        __call__(inputs): Wrapper for _call()\n    \"\"\"\n    def __init__(self, **kwargs):\n        allowed_kwargs = {'name', 'logging'}\n        for kwarg in kwargs.keys():\n            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n        name = kwargs.get('name')\n        if not name:\n            layer = self.__class__.__name__.lower()\n            name = layer + '_' + str(get_layer_uid(layer))\n        self.name = name\n        self.vars = {}\n        logging = kwargs.get('logging', False)\n        self.logging = logging\n        self.issparse = False\n\n    def _call(self, inputs):\n        return inputs\n\n    def __call__(self, inputs):\n        with tf.name_scope(self.name):\n            outputs = self._call(inputs)\n            return outputs\n\nclass GraphConvolution(Layer):\n    \"\"\"Basic graph convolution layer for undirected graph without edge labels.\"\"\"\n    def __init__(self, input_dim, output_dim, adj, dropout=0., act=tf.nn.relu, **kwargs):\n        super(GraphConvolution, self).__init__(**kwargs)\n        with tf.variable_scope(self.name + '_vars'):\n            self.vars['weights'] = weight_variable_glorot(input_dim, output_dim, name=\"weights\")\n        self.dropout = dropout\n        self.adj = adj\n        self.act = act\n\n    def _call(self, inputs):\n        x = inputs\n        x = tf.nn.dropout(x, 1-self.dropout)\n        x = tf.matmul(x, self.vars['weights'])\n        x = tf.sparse_tensor_dense_matmul(self.adj, x)\n        outputs = self.act(x)\n        return outputs\nclass GraphConvolutionSparse(Layer):\n    \"\"\"Graph convolution layer for sparse inputs.\"\"\"\n    def __init__(self, input_dim, output_dim, adj, features_nonzero, dropout=0., act=tf.nn.relu, **kwargs):\n        super(GraphConvolutionSparse, self).__init__(**kwargs)\n        with tf.variable_scope(self.name + '_vars'):\n            self.vars['weights'] = weight_variable_glorot(input_dim, output_dim, name=\"weights\")\n        self.dropout = dropout\n        self.adj = adj\n        self.act = act\n        self.issparse = True\n        self.features_nonzero = features_nonzero\n\n    def _call(self, inputs):\n        x = inputs\n        x = dropout_sparse(x, 1-self.dropout, self.features_nonzero)\n        x = tf.sparse_tensor_dense_matmul(x, self.vars['weights'])\n        x = tf.sparse_tensor_dense_matmul(self.adj, x)\n        outputs = self.act(x)\n        return outputs\nclass InnerProductDecoder(Layer):\n    \"\"\"Decoder model layer for link prediction.\"\"\"\n    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n        super(InnerProductDecoder, self).__init__(**kwargs)\n        self.dropout = dropout\n        self.act = act\n\n    def _call(self, inputs):\n        inputs = tf.nn.dropout(inputs, 1-self.dropout)\n        x = tf.transpose(inputs)\n        x = tf.matmul(inputs, x)\n        x = tf.reshape(x, [-1])\n        outputs = self.act(x)\n        return outputs","metadata":{"id":"PU-sY15ydWcF","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:15:18.519871Z","iopub.execute_input":"2025-03-15T17:15:18.520179Z","iopub.status.idle":"2025-03-15T17:15:18.534233Z","shell.execute_reply.started":"2025-03-15T17:15:18.520151Z","shell.execute_reply":"2025-03-15T17:15:18.533677Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"class Model(object):\n    def __init__(self, **kwargs):\n        allowed_kwargs = {'name', 'logging'}\n        for kwarg in kwargs.keys():\n            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n\n        for kwarg in kwargs.keys():\n            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n        name = kwargs.get('name')\n        if not name:\n            name = self.__class__.__name__.lower()\n        self.name = name\n\n        logging = kwargs.get('logging', False)\n        self.logging = logging\n\n        self.vars = {}\n\n    def _build(self):\n        raise NotImplementedError\n\n    def build(self):\n        \"\"\" Wrapper for _build() \"\"\"\n        with tf.variable_scope(self.name):\n            self._build()\n        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n        self.vars = {var.name: var for var in variables}\n\n    def fit(self):\n        pass\n\n    def predict(self):\n        pass\n\nclass GCNModelAE(Model):\n    def __init__(self, placeholders, num_features, features_nonzero, **kwargs):\n        super(GCNModelAE, self).__init__(**kwargs)\n\n        self.inputs = placeholders['features']\n        self.input_dim = num_features\n        self.features_nonzero = features_nonzero\n        self.adj = placeholders['adj']\n        self.dropout = placeholders['dropout']\n        self.build()\n\n    def _build(self):\n        self.hidden1 = GraphConvolutionSparse(input_dim=self.input_dim,\n                                              output_dim=128,\n                                              adj=self.adj,\n                                              features_nonzero=self.features_nonzero,\n                                              act=tf.nn.relu,\n                                              dropout=self.dropout,\n                                              logging=self.logging)(self.inputs)\n\n        self.embeddings = GraphConvolution(input_dim=128,\n                                           output_dim=16,\n                                           adj=self.adj,\n                                           act=lambda x: x,\n                                           dropout=self.dropout,\n                                           logging=self.logging)(self.hidden1)\n\n        self.z_mean = self.embeddings\n\n        self.reconstructions = InnerProductDecoder(input_dim=16,\n                                      act=lambda x: x,\n                                      logging=self.logging)(self.embeddings)","metadata":{"id":"3wy9xu_7de--","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:15:18.535055Z","iopub.execute_input":"2025-03-15T17:15:18.535255Z","iopub.status.idle":"2025-03-15T17:15:18.552730Z","shell.execute_reply.started":"2025-03-15T17:15:18.535233Z","shell.execute_reply":"2025-03-15T17:15:18.551941Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"class OptimizerAE(object):\n    def __init__(self, preds, labels, pos_weight, norm):\n        preds_sub = preds\n        labels_sub = labels\n\n        self.cost = norm * tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=preds_sub, targets=labels_sub, pos_weight=pos_weight))\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.01)  # Adam Optimizer\n\n        self.opt_op = self.optimizer.minimize(self.cost)\n        self.grads_vars = self.optimizer.compute_gradients(self.cost)\n\n        self.correct_prediction = tf.equal(tf.cast(tf.greater_equal(tf.sigmoid(preds_sub), 0.5), tf.int32),\n                                           tf.cast(labels_sub, tf.int32))\n        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))","metadata":{"id":"9lULmLcpdoJA","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:15:18.657457Z","iopub.execute_input":"2025-03-15T17:15:18.657718Z","iopub.status.idle":"2025-03-15T17:15:18.663329Z","shell.execute_reply.started":"2025-03-15T17:15:18.657694Z","shell.execute_reply":"2025-03-15T17:15:18.662397Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def sparse_to_tuple(sparse_mx):\n    if not sp.isspmatrix_coo(sparse_mx):\n        sparse_mx = sparse_mx.tocoo()\n    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n    values = sparse_mx.data\n    shape = sparse_mx.shape\n    return coords, values, shape\n\n\ndef preprocess_graph(adj):\n    adj = sp.coo_matrix(adj)\n    adj_ = adj + sp.eye(adj.shape[0])\n    rowsum = np.array(adj_.sum(1))\n    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n    return sparse_to_tuple(adj_normalized)\n\n\ndef construct_feed_dict(adj_normalized, adj, features, placeholders):\n    # construct feed dictionary\n    feed_dict = dict()\n    feed_dict.update({placeholders['features']: features})\n    feed_dict.update({placeholders['adj']: adj_normalized})\n    feed_dict.update({placeholders['adj_orig']: adj})\n    return feed_dict\n\n\ndef mask_test_edges(adj):\n    # Function to build test set with 10% positive links\n    # NOTE: Splits are randomized and results might slightly deviate from reported numbers in the paper.\n    # TODO: Clean up.\n\n    # Remove diagonal elements\n    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n    adj.eliminate_zeros()\n    # Check that diag is zero:\n    assert np.diag(adj.todense()).sum() == 0\n\n    adj_triu = sp.triu(adj)\n    adj_tuple = sparse_to_tuple(adj_triu)\n    edges = adj_tuple[0]\n    edges_all = sparse_to_tuple(adj)[0]\n    num_test = int(np.floor(edges.shape[0] / 10.))\n    num_val = int(np.floor(edges.shape[0] / 20.))\n\n    all_edge_idx = list(range(edges.shape[0]))\n    np.random.shuffle(all_edge_idx)\n    val_edge_idx = all_edge_idx[:num_val]\n    test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n    test_edges = edges[test_edge_idx]\n    val_edges = edges[val_edge_idx]\n    train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n\n    def ismember(a, b, tol=5):\n        rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n        return np.any(rows_close)\n\n    test_edges_false = []\n    while len(test_edges_false) < len(test_edges):\n        idx_i = np.random.randint(0, adj.shape[0])\n        idx_j = np.random.randint(0, adj.shape[0])\n        if idx_i == idx_j:\n            continue\n        if ismember([idx_i, idx_j], edges_all):\n            continue\n        if test_edges_false:\n            if ismember([idx_j, idx_i], np.array(test_edges_false)):\n                continue\n            if ismember([idx_i, idx_j], np.array(test_edges_false)):\n                continue\n        test_edges_false.append([idx_i, idx_j])\n\n    val_edges_false = []\n    while len(val_edges_false) < len(val_edges):\n        idx_i = np.random.randint(0, adj.shape[0])\n        idx_j = np.random.randint(0, adj.shape[0])\n        if idx_i == idx_j:\n            continue\n        if ismember([idx_i, idx_j], train_edges):\n            continue\n        if ismember([idx_j, idx_i], train_edges):\n            continue\n        if ismember([idx_i, idx_j], val_edges):\n            continue\n        if ismember([idx_j, idx_i], val_edges):\n            continue\n        if val_edges_false:\n            if ismember([idx_j, idx_i], np.array(val_edges_false)):\n                continue\n            if ismember([idx_i, idx_j], np.array(val_edges_false)):\n                continue\n        val_edges_false.append([idx_i, idx_j])\n\n    #assert ~ismember(test_edges_false, edges_all)\n    #assert ~ismember(val_edges_false, edges_all)\n    #assert ~ismember(val_edges, train_edges)\n    #assert ~ismember(test_edges, train_edges)\n    #assert ~ismember(val_edges, test_edges)\n\n    data = np.ones(train_edges.shape[0])\n\n    # Re-build adj matrix\n    adj_train = sp.csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n    adj_train = adj_train + adj_train.T\n\n    # NOTE: these edge lists only contain single direction of edge!\n    return adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false","metadata":{"id":"QLF6j8X7dtf-","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:15:18.664532Z","iopub.execute_input":"2025-03-15T17:15:18.664791Z","iopub.status.idle":"2025-03-15T17:15:18.677699Z","shell.execute_reply.started":"2025-03-15T17:15:18.664772Z","shell.execute_reply":"2025-03-15T17:15:18.676907Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Load data\nadj = pd.read_csv('DDR-HC-new.csv')\nfeatures = pd.read_csv('all_drug_data_processed.csv')","metadata":{"id":"HlHWXpDAeKlT","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:15:18.679381Z","iopub.execute_input":"2025-03-15T17:15:18.679585Z","iopub.status.idle":"2025-03-15T17:15:18.718824Z","shell.execute_reply.started":"2025-03-15T17:15:18.679567Z","shell.execute_reply":"2025-03-15T17:15:18.717945Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"#feature_matrix = feature_matrix.drop('Unnamed: 0',axis=1)\nfeatures.set_index('Name',inplace=True)\nadj.set_index('Unnamed: 0', inplace=True)","metadata":{"id":"vXLdyqodjaJb","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:15:18.720011Z","iopub.execute_input":"2025-03-15T17:15:18.720264Z","iopub.status.idle":"2025-03-15T17:15:18.731686Z","shell.execute_reply.started":"2025-03-15T17:15:18.720244Z","shell.execute_reply":"2025-03-15T17:15:18.730977Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"features = sp.csr_matrix(features.astype(pd.SparseDtype(\"float64\",0)).sparse.to_coo())\nadj = sp.csr_matrix(adj.astype(pd.SparseDtype(\"float64\",0)))\ns = StandardScaler(with_mean=False)\nfeatures = s.fit_transform(features)","metadata":{"id":"VenC_Ikgjhw3","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:15:18.732599Z","iopub.execute_input":"2025-03-15T17:15:18.732856Z","iopub.status.idle":"2025-03-15T17:15:18.858387Z","shell.execute_reply.started":"2025-03-15T17:15:18.732826Z","shell.execute_reply":"2025-03-15T17:15:18.857575Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# Store original adjacency matrix (without diagonal entries) for later\nadj_orig = adj\nadj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\nadj_orig.eliminate_zeros()\n\n#adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n#adj = adj_train\nadj = adj_orig\n\n# Some preprocessing\nadj_norm = preprocess_graph(adj)\n\n# Define placeholders\nplaceholders = {\n    'features': tf.sparse_placeholder(tf.float32),\n    'adj': tf.sparse_placeholder(tf.float32),\n    'adj_orig': tf.sparse_placeholder(tf.float32),\n    'dropout': tf.placeholder_with_default(0., shape=())\n}\n\nnum_nodes = adj.shape[0]\n\nfeatures = sparse_to_tuple(features.tocoo())\nnum_features = features[2][1]\nfeatures_nonzero = features[1].shape[0]","metadata":{"id":"97CStYwKeObQ","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:15:18.859419Z","iopub.execute_input":"2025-03-15T17:15:18.859698Z","iopub.status.idle":"2025-03-15T17:15:18.873624Z","shell.execute_reply.started":"2025-03-15T17:15:18.859671Z","shell.execute_reply":"2025-03-15T17:15:18.873051Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Create model\nmodel = None\nmodel = GCNModelAE(placeholders, num_features, features_nonzero)\npos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\nnorm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n\n# Optimizer\nwith tf.name_scope('optimizer'):\n  opt = OptimizerAE(preds=model.reconstructions,labels=tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'],validate_indices=False), [-1]),pos_weight=pos_weight,norm=norm)","metadata":{"id":"4WOVNBmOeUMa","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:15:18.874392Z","iopub.execute_input":"2025-03-15T17:15:18.874616Z","iopub.status.idle":"2025-03-15T17:15:19.130844Z","shell.execute_reply.started":"2025-03-15T17:15:18.874591Z","shell.execute_reply":"2025-03-15T17:15:19.129943Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# Initialize session\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\ncost_val = []\nacc_val = []\ndef get_roc_score(edges_pos, edges_neg, emb=None):\n    if emb is None:\n        feed_dict.update({placeholders['dropout']: 0})\n        emb = sess.run(model.z_mean, feed_dict=feed_dict)\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n\n    # Predict on test set of edges\n    adj_rec = np.dot(emb, emb.T)\n    preds = []\n    pos = []\n    for e in edges_pos:\n        preds.append(sigmoid(adj_rec[e[0], e[1]]))\n        pos.append(adj_orig[e[0], e[1]])\n\n    preds_neg = []\n    neg = []\n    for e in edges_neg:\n        preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n        neg.append(adj_orig[e[0], e[1]])\n\n    preds_all = np.hstack([preds, preds_neg])\n    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n    roc_score = roc_auc_score(labels_all, preds_all)\n    ap_score = average_precision_score(labels_all, preds_all)\n\n    return roc_score, ap_score","metadata":{"id":"6MZgZ7sHeYTS","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:15:19.133075Z","iopub.execute_input":"2025-03-15T17:15:19.133297Z","iopub.status.idle":"2025-03-15T17:15:19.212091Z","shell.execute_reply.started":"2025-03-15T17:15:19.133276Z","shell.execute_reply":"2025-03-15T17:15:19.211223Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"cost_val = []\nacc_val = []\nval_roc_score = []\n\nadj_label = adj + sp.eye(adj.shape[0])\nadj_label = sparse_to_tuple(adj_label)\n\n# Train model\nfor epoch in range(500):\n\n    t = time.time()\n    # Construct feed dictionary\n    feed_dict = construct_feed_dict(adj_norm, adj_label, features, placeholders)\n    feed_dict.update({placeholders['dropout']: 0.})\n    # Run single weight update\n    outs = sess.run([opt.opt_op, opt.cost, opt.accuracy], feed_dict=feed_dict)\n\n    # Compute average loss\n    avg_cost = outs[1]\n    avg_accuracy = outs[2]\n    cost_val.append(avg_cost)\n    #roc_curr, ap_curr = get_roc_score(val_edges, val_edges_false)\n    #val_roc_score.append(roc_curr)\n\n    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(avg_cost),\n          \"time=\", \"{:.5f}\".format(time.time() - t))\n\nprint(\"Optimization Finished!\")\nemb = sess.run(model.z_mean, feed_dict = feed_dict)\n#roc_score, ap_score = get_roc_score(test_edges, test_edges_false)\n#print('Test ROC score: ' + str(roc_score))\n#print('Test AP score: ' + str(ap_score))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9BF3f7oje6dv","outputId":"4cbecf40-8898-4d97-9be1-2ddacac78c3f","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:15:19.213161Z","iopub.execute_input":"2025-03-15T17:15:19.213445Z","iopub.status.idle":"2025-03-15T17:15:22.896365Z","shell.execute_reply.started":"2025-03-15T17:15:19.213417Z","shell.execute_reply":"2025-03-15T17:15:22.895666Z"}},"outputs":[{"name":"stdout","text":"Epoch: 0001 train_loss= 1.81696 time= 1.61170\nEpoch: 0002 train_loss= 0.70993 time= 0.00414\nEpoch: 0003 train_loss= 1.30345 time= 0.00347\nEpoch: 0004 train_loss= 0.94801 time= 0.00473\nEpoch: 0005 train_loss= 0.70931 time= 0.00367\nEpoch: 0006 train_loss= 0.66247 time= 0.00417\nEpoch: 0007 train_loss= 0.66340 time= 0.00413\nEpoch: 0008 train_loss= 0.67004 time= 0.00443\nEpoch: 0009 train_loss= 0.67536 time= 0.00308\nEpoch: 0010 train_loss= 0.67751 time= 0.00299\nEpoch: 0011 train_loss= 0.67693 time= 0.00333\nEpoch: 0012 train_loss= 0.67344 time= 0.00327\nEpoch: 0013 train_loss= 0.66764 time= 0.00377\nEpoch: 0014 train_loss= 0.66069 time= 0.00416\nEpoch: 0015 train_loss= 0.65227 time= 0.00351\nEpoch: 0016 train_loss= 0.64361 time= 0.00306\nEpoch: 0017 train_loss= 0.63358 time= 0.00380\nEpoch: 0018 train_loss= 0.62489 time= 0.00342\nEpoch: 0019 train_loss= 0.61118 time= 0.00305\nEpoch: 0020 train_loss= 0.59675 time= 0.00324\nEpoch: 0021 train_loss= 0.58339 time= 0.00295\nEpoch: 0022 train_loss= 0.56970 time= 0.00320\nEpoch: 0023 train_loss= 0.55775 time= 0.00302\nEpoch: 0024 train_loss= 0.54790 time= 0.00393\nEpoch: 0025 train_loss= 0.53849 time= 0.00423\nEpoch: 0026 train_loss= 0.52932 time= 0.00348\nEpoch: 0027 train_loss= 0.52148 time= 0.00407\nEpoch: 0028 train_loss= 0.51550 time= 0.00438\nEpoch: 0029 train_loss= 0.51033 time= 0.00322\nEpoch: 0030 train_loss= 0.50542 time= 0.00385\nEpoch: 0031 train_loss= 0.50070 time= 0.00334\nEpoch: 0032 train_loss= 0.49582 time= 0.00439\nEpoch: 0033 train_loss= 0.49074 time= 0.00335\nEpoch: 0034 train_loss= 0.48586 time= 0.00316\nEpoch: 0035 train_loss= 0.48104 time= 0.00279\nEpoch: 0036 train_loss= 0.47620 time= 0.00310\nEpoch: 0037 train_loss= 0.47177 time= 0.00316\nEpoch: 0038 train_loss= 0.46805 time= 0.00297\nEpoch: 0039 train_loss= 0.46487 time= 0.00310\nEpoch: 0040 train_loss= 0.46200 time= 0.00301\nEpoch: 0041 train_loss= 0.45933 time= 0.00417\nEpoch: 0042 train_loss= 0.45689 time= 0.00319\nEpoch: 0043 train_loss= 0.45455 time= 0.00297\nEpoch: 0044 train_loss= 0.45217 time= 0.00313\nEpoch: 0045 train_loss= 0.44972 time= 0.00352\nEpoch: 0046 train_loss= 0.44738 time= 0.00361\nEpoch: 0047 train_loss= 0.44507 time= 0.00318\nEpoch: 0048 train_loss= 0.44273 time= 0.00295\nEpoch: 0049 train_loss= 0.44041 time= 0.00287\nEpoch: 0050 train_loss= 0.43830 time= 0.00301\nEpoch: 0051 train_loss= 0.43628 time= 0.00362\nEpoch: 0052 train_loss= 0.43424 time= 0.00325\nEpoch: 0053 train_loss= 0.43217 time= 0.00340\nEpoch: 0054 train_loss= 0.43010 time= 0.00321\nEpoch: 0055 train_loss= 0.42810 time= 0.00326\nEpoch: 0056 train_loss= 0.42622 time= 0.00305\nEpoch: 0057 train_loss= 0.42449 time= 0.00285\nEpoch: 0058 train_loss= 0.42286 time= 0.00311\nEpoch: 0059 train_loss= 0.42127 time= 0.00308\nEpoch: 0060 train_loss= 0.41968 time= 0.00301\nEpoch: 0061 train_loss= 0.41828 time= 0.00359\nEpoch: 0062 train_loss= 0.41687 time= 0.00414\nEpoch: 0063 train_loss= 0.41533 time= 0.00281\nEpoch: 0064 train_loss= 0.41375 time= 0.00317\nEpoch: 0065 train_loss= 0.41221 time= 0.00306\nEpoch: 0066 train_loss= 0.41074 time= 0.00416\nEpoch: 0067 train_loss= 0.40927 time= 0.00291\nEpoch: 0068 train_loss= 0.40779 time= 0.00305\nEpoch: 0069 train_loss= 0.40633 time= 0.00301\nEpoch: 0070 train_loss= 0.40486 time= 0.00319\nEpoch: 0071 train_loss= 0.40345 time= 0.00287\nEpoch: 0072 train_loss= 0.40210 time= 0.00295\nEpoch: 0073 train_loss= 0.40073 time= 0.00295\nEpoch: 0074 train_loss= 0.39945 time= 0.00307\nEpoch: 0075 train_loss= 0.39822 time= 0.00295\nEpoch: 0076 train_loss= 0.39698 time= 0.00325\nEpoch: 0077 train_loss= 0.39569 time= 0.00289\nEpoch: 0078 train_loss= 0.39441 time= 0.00299\nEpoch: 0079 train_loss= 0.39319 time= 0.00347\nEpoch: 0080 train_loss= 0.39197 time= 0.00303\nEpoch: 0081 train_loss= 0.39075 time= 0.00307\nEpoch: 0082 train_loss= 0.38962 time= 0.00322\nEpoch: 0083 train_loss= 0.38853 time= 0.00328\nEpoch: 0084 train_loss= 0.38747 time= 0.00387\nEpoch: 0085 train_loss= 0.38650 time= 0.00345\nEpoch: 0086 train_loss= 0.38554 time= 0.00343\nEpoch: 0087 train_loss= 0.38462 time= 0.00305\nEpoch: 0088 train_loss= 0.38366 time= 0.00322\nEpoch: 0089 train_loss= 0.38277 time= 0.00296\nEpoch: 0090 train_loss= 0.38189 time= 0.00310\nEpoch: 0091 train_loss= 0.38104 time= 0.00299\nEpoch: 0092 train_loss= 0.38023 time= 0.00342\nEpoch: 0093 train_loss= 0.37940 time= 0.00412\nEpoch: 0094 train_loss= 0.37859 time= 0.00288\nEpoch: 0095 train_loss= 0.37782 time= 0.00308\nEpoch: 0096 train_loss= 0.37707 time= 0.00333\nEpoch: 0097 train_loss= 0.37631 time= 0.00333\nEpoch: 0098 train_loss= 0.37559 time= 0.00307\nEpoch: 0099 train_loss= 0.37486 time= 0.00287\nEpoch: 0100 train_loss= 0.37416 time= 0.00286\nEpoch: 0101 train_loss= 0.37343 time= 0.00310\nEpoch: 0102 train_loss= 0.37274 time= 0.00312\nEpoch: 0103 train_loss= 0.37206 time= 0.00298\nEpoch: 0104 train_loss= 0.37141 time= 0.00349\nEpoch: 0105 train_loss= 0.37075 time= 0.00299\nEpoch: 0106 train_loss= 0.37009 time= 0.00404\nEpoch: 0107 train_loss= 0.36945 time= 0.00314\nEpoch: 0108 train_loss= 0.36882 time= 0.00360\nEpoch: 0109 train_loss= 0.36816 time= 0.00369\nEpoch: 0110 train_loss= 0.36751 time= 0.00311\nEpoch: 0111 train_loss= 0.36689 time= 0.00324\nEpoch: 0112 train_loss= 0.36627 time= 0.00409\nEpoch: 0113 train_loss= 0.36568 time= 0.00313\nEpoch: 0114 train_loss= 0.36507 time= 0.00313\nEpoch: 0115 train_loss= 0.36447 time= 0.00320\nEpoch: 0116 train_loss= 0.36388 time= 0.00309\nEpoch: 0117 train_loss= 0.36331 time= 0.00402\nEpoch: 0118 train_loss= 0.36269 time= 0.00348\nEpoch: 0119 train_loss= 0.36212 time= 0.00358\nEpoch: 0120 train_loss= 0.36151 time= 0.00327\nEpoch: 0121 train_loss= 0.36092 time= 0.00310\nEpoch: 0122 train_loss= 0.36032 time= 0.00341\nEpoch: 0123 train_loss= 0.35977 time= 0.00296\nEpoch: 0124 train_loss= 0.35921 time= 0.00294\nEpoch: 0125 train_loss= 0.35863 time= 0.00337\nEpoch: 0126 train_loss= 0.35805 time= 0.00325\nEpoch: 0127 train_loss= 0.35749 time= 0.00307\nEpoch: 0128 train_loss= 0.35695 time= 0.00364\nEpoch: 0129 train_loss= 0.35637 time= 0.00293\nEpoch: 0130 train_loss= 0.35578 time= 0.00293\nEpoch: 0131 train_loss= 0.35521 time= 0.00301\nEpoch: 0132 train_loss= 0.35464 time= 0.00315\nEpoch: 0133 train_loss= 0.35407 time= 0.00409\nEpoch: 0134 train_loss= 0.35353 time= 0.00321\nEpoch: 0135 train_loss= 0.35294 time= 0.00298\nEpoch: 0136 train_loss= 0.35235 time= 0.00402\nEpoch: 0137 train_loss= 0.35179 time= 0.00324\nEpoch: 0138 train_loss= 0.35122 time= 0.00323\nEpoch: 0139 train_loss= 0.35082 time= 0.00310\nEpoch: 0140 train_loss= 0.35014 time= 0.00288\nEpoch: 0141 train_loss= 0.34970 time= 0.00327\nEpoch: 0142 train_loss= 0.34929 time= 0.00341\nEpoch: 0143 train_loss= 0.34850 time= 0.00346\nEpoch: 0144 train_loss= 0.34777 time= 0.00317\nEpoch: 0145 train_loss= 0.34734 time= 0.00288\nEpoch: 0146 train_loss= 0.34673 time= 0.00313\nEpoch: 0147 train_loss= 0.34621 time= 0.00403\nEpoch: 0148 train_loss= 0.34569 time= 0.00406\nEpoch: 0149 train_loss= 0.34502 time= 0.00326\nEpoch: 0150 train_loss= 0.34427 time= 0.00364\nEpoch: 0151 train_loss= 0.34372 time= 0.00375\nEpoch: 0152 train_loss= 0.34321 time= 0.00393\nEpoch: 0153 train_loss= 0.34273 time= 0.00302\nEpoch: 0154 train_loss= 0.34222 time= 0.00289\nEpoch: 0155 train_loss= 0.34173 time= 0.00289\nEpoch: 0156 train_loss= 0.34123 time= 0.00317\nEpoch: 0157 train_loss= 0.34072 time= 0.00301\nEpoch: 0158 train_loss= 0.34001 time= 0.00315\nEpoch: 0159 train_loss= 0.33928 time= 0.00325\nEpoch: 0160 train_loss= 0.33861 time= 0.00322\nEpoch: 0161 train_loss= 0.33806 time= 0.00298\nEpoch: 0162 train_loss= 0.33760 time= 0.00287\nEpoch: 0163 train_loss= 0.33734 time= 0.00279\nEpoch: 0164 train_loss= 0.33714 time= 0.00290\nEpoch: 0165 train_loss= 0.33686 time= 0.00330\nEpoch: 0166 train_loss= 0.33662 time= 0.00357\nEpoch: 0167 train_loss= 0.33608 time= 0.00317\nEpoch: 0168 train_loss= 0.33483 time= 0.00293\nEpoch: 0169 train_loss= 0.33361 time= 0.00330\nEpoch: 0170 train_loss= 0.33354 time= 0.00383\nEpoch: 0171 train_loss= 0.33354 time= 0.00286\nEpoch: 0172 train_loss= 0.33241 time= 0.00298\nEpoch: 0173 train_loss= 0.33120 time= 0.00303\nEpoch: 0174 train_loss= 0.33060 time= 0.00318\nEpoch: 0175 train_loss= 0.33044 time= 0.00289\nEpoch: 0176 train_loss= 0.33031 time= 0.00290\nEpoch: 0177 train_loss= 0.32935 time= 0.00306\nEpoch: 0178 train_loss= 0.32836 time= 0.00313\nEpoch: 0179 train_loss= 0.32752 time= 0.00423\nEpoch: 0180 train_loss= 0.32695 time= 0.00300\nEpoch: 0181 train_loss= 0.32659 time= 0.00314\nEpoch: 0182 train_loss= 0.32634 time= 0.00319\nEpoch: 0183 train_loss= 0.32619 time= 0.00333\nEpoch: 0184 train_loss= 0.32556 time= 0.00323\nEpoch: 0185 train_loss= 0.32483 time= 0.00309\nEpoch: 0186 train_loss= 0.32410 time= 0.00281\nEpoch: 0187 train_loss= 0.32480 time= 0.00343\nEpoch: 0188 train_loss= 0.32680 time= 0.00286\nEpoch: 0189 train_loss= 0.32959 time= 0.00299\nEpoch: 0190 train_loss= 0.32718 time= 0.00410\nEpoch: 0191 train_loss= 0.32451 time= 0.00359\nEpoch: 0192 train_loss= 0.32078 time= 0.00348\nEpoch: 0193 train_loss= 0.32296 time= 0.00401\nEpoch: 0194 train_loss= 0.32574 time= 0.00368\nEpoch: 0195 train_loss= 0.31942 time= 0.00298\nEpoch: 0196 train_loss= 0.32020 time= 0.00286\nEpoch: 0197 train_loss= 0.32362 time= 0.00300\nEpoch: 0198 train_loss= 0.31906 time= 0.00445\nEpoch: 0199 train_loss= 0.31830 time= 0.00406\nEpoch: 0200 train_loss= 0.31871 time= 0.00307\nEpoch: 0201 train_loss= 0.31781 time= 0.00304\nEpoch: 0202 train_loss= 0.31705 time= 0.00409\nEpoch: 0203 train_loss= 0.31472 time= 0.00399\nEpoch: 0204 train_loss= 0.31567 time= 0.00309\nEpoch: 0205 train_loss= 0.31567 time= 0.00489\nEpoch: 0206 train_loss= 0.31355 time= 0.00399\nEpoch: 0207 train_loss= 0.31356 time= 0.00406\nEpoch: 0208 train_loss= 0.31235 time= 0.00408\nEpoch: 0209 train_loss= 0.31196 time= 0.00290\nEpoch: 0210 train_loss= 0.31287 time= 0.00327\nEpoch: 0211 train_loss= 0.31165 time= 0.00333\nEpoch: 0212 train_loss= 0.31140 time= 0.00298\nEpoch: 0213 train_loss= 0.31071 time= 0.00298\nEpoch: 0214 train_loss= 0.30926 time= 0.00337\nEpoch: 0215 train_loss= 0.30903 time= 0.00322\nEpoch: 0216 train_loss= 0.30834 time= 0.00302\nEpoch: 0217 train_loss= 0.30758 time= 0.00302\nEpoch: 0218 train_loss= 0.30748 time= 0.00303\nEpoch: 0219 train_loss= 0.30694 time= 0.00289\nEpoch: 0220 train_loss= 0.30640 time= 0.00315\nEpoch: 0221 train_loss= 0.30705 time= 0.00311\nEpoch: 0222 train_loss= 0.30812 time= 0.00295\nEpoch: 0223 train_loss= 0.31412 time= 0.00292\nEpoch: 0224 train_loss= 0.31604 time= 0.00311\nEpoch: 0225 train_loss= 0.32513 time= 0.00323\nEpoch: 0226 train_loss= 0.30951 time= 0.00402\nEpoch: 0227 train_loss= 0.30720 time= 0.00299\nEpoch: 0228 train_loss= 0.31669 time= 0.00332\nEpoch: 0229 train_loss= 0.31108 time= 0.00384\nEpoch: 0230 train_loss= 0.30384 time= 0.00312\nEpoch: 0231 train_loss= 0.31298 time= 0.00304\nEpoch: 0232 train_loss= 0.30650 time= 0.00437\nEpoch: 0233 train_loss= 0.30442 time= 0.00422\nEpoch: 0234 train_loss= 0.30586 time= 0.00392\nEpoch: 0235 train_loss= 0.30450 time= 0.00378\nEpoch: 0236 train_loss= 0.30312 time= 0.00362\nEpoch: 0237 train_loss= 0.30129 time= 0.00350\nEpoch: 0238 train_loss= 0.30361 time= 0.00407\nEpoch: 0239 train_loss= 0.30200 time= 0.00328\nEpoch: 0240 train_loss= 0.30137 time= 0.00335\nEpoch: 0241 train_loss= 0.29929 time= 0.00389\nEpoch: 0242 train_loss= 0.30262 time= 0.00377\nEpoch: 0243 train_loss= 0.30086 time= 0.00390\nEpoch: 0244 train_loss= 0.29875 time= 0.00305\nEpoch: 0245 train_loss= 0.29843 time= 0.00323\nEpoch: 0246 train_loss= 0.29835 time= 0.00363\nEpoch: 0247 train_loss= 0.30080 time= 0.00342\nEpoch: 0248 train_loss= 0.29721 time= 0.00344\nEpoch: 0249 train_loss= 0.29671 time= 0.00311\nEpoch: 0250 train_loss= 0.29627 time= 0.00334\nEpoch: 0251 train_loss= 0.29719 time= 0.00350\nEpoch: 0252 train_loss= 0.29648 time= 0.00307\nEpoch: 0253 train_loss= 0.29512 time= 0.00328\nEpoch: 0254 train_loss= 0.29460 time= 0.00334\nEpoch: 0255 train_loss= 0.29368 time= 0.00315\nEpoch: 0256 train_loss= 0.29437 time= 0.00313\nEpoch: 0257 train_loss= 0.29420 time= 0.00298\nEpoch: 0258 train_loss= 0.29384 time= 0.00306\nEpoch: 0259 train_loss= 0.29331 time= 0.00354\nEpoch: 0260 train_loss= 0.29205 time= 0.00381\nEpoch: 0261 train_loss= 0.29201 time= 0.00353\nEpoch: 0262 train_loss= 0.29112 time= 0.00339\nEpoch: 0263 train_loss= 0.29120 time= 0.00323\nEpoch: 0264 train_loss= 0.29125 time= 0.00340\nEpoch: 0265 train_loss= 0.29095 time= 0.00328\nEpoch: 0266 train_loss= 0.29189 time= 0.00344\nEpoch: 0267 train_loss= 0.29210 time= 0.00331\nEpoch: 0268 train_loss= 0.29586 time= 0.00365\nEpoch: 0269 train_loss= 0.29385 time= 0.00333\nEpoch: 0270 train_loss= 0.29556 time= 0.00350\nEpoch: 0271 train_loss= 0.29234 time= 0.00349\nEpoch: 0272 train_loss= 0.28956 time= 0.00352\nEpoch: 0273 train_loss= 0.28808 time= 0.00387\nEpoch: 0274 train_loss= 0.28868 time= 0.00469\nEpoch: 0275 train_loss= 0.29025 time= 0.00390\nEpoch: 0276 train_loss= 0.29090 time= 0.00340\nEpoch: 0277 train_loss= 0.29476 time= 0.00453\nEpoch: 0278 train_loss= 0.29071 time= 0.00394\nEpoch: 0279 train_loss= 0.28845 time= 0.00376\nEpoch: 0280 train_loss= 0.28645 time= 0.00352\nEpoch: 0281 train_loss= 0.28629 time= 0.00340\nEpoch: 0282 train_loss= 0.28619 time= 0.00385\nEpoch: 0283 train_loss= 0.28645 time= 0.00343\nEpoch: 0284 train_loss= 0.28882 time= 0.00366\nEpoch: 0285 train_loss= 0.28866 time= 0.00398\nEpoch: 0286 train_loss= 0.29235 time= 0.00396\nEpoch: 0287 train_loss= 0.29335 time= 0.00426\nEpoch: 0288 train_loss= 0.29918 time= 0.00378\nEpoch: 0289 train_loss= 0.28841 time= 0.00326\nEpoch: 0290 train_loss= 0.28461 time= 0.00302\nEpoch: 0291 train_loss= 0.28436 time= 0.00458\nEpoch: 0292 train_loss= 0.28747 time= 0.00408\nEpoch: 0293 train_loss= 0.29366 time= 0.00380\nEpoch: 0294 train_loss= 0.28761 time= 0.00312\nEpoch: 0295 train_loss= 0.28342 time= 0.00459\nEpoch: 0296 train_loss= 0.28648 time= 0.00420\nEpoch: 0297 train_loss= 0.28978 time= 0.00482\nEpoch: 0298 train_loss= 0.28941 time= 0.00480\nEpoch: 0299 train_loss= 0.28277 time= 0.00440\nEpoch: 0300 train_loss= 0.28265 time= 0.00482\nEpoch: 0301 train_loss= 0.28635 time= 0.00381\nEpoch: 0302 train_loss= 0.28448 time= 0.00348\nEpoch: 0303 train_loss= 0.28331 time= 0.00366\nEpoch: 0304 train_loss= 0.28103 time= 0.00343\nEpoch: 0305 train_loss= 0.28075 time= 0.00391\nEpoch: 0306 train_loss= 0.28194 time= 0.00369\nEpoch: 0307 train_loss= 0.28226 time= 0.00342\nEpoch: 0308 train_loss= 0.28297 time= 0.00375\nEpoch: 0309 train_loss= 0.28095 time= 0.00468\nEpoch: 0310 train_loss= 0.28003 time= 0.00471\nEpoch: 0311 train_loss= 0.27867 time= 0.00469\nEpoch: 0312 train_loss= 0.27827 time= 0.00445\nEpoch: 0313 train_loss= 0.27885 time= 0.00358\nEpoch: 0314 train_loss= 0.27937 time= 0.00360\nEpoch: 0315 train_loss= 0.28134 time= 0.00362\nEpoch: 0316 train_loss= 0.28054 time= 0.00377\nEpoch: 0317 train_loss= 0.28100 time= 0.00351\nEpoch: 0318 train_loss= 0.27915 time= 0.00355\nEpoch: 0319 train_loss= 0.27832 time= 0.00367\nEpoch: 0320 train_loss= 0.27700 time= 0.00480\nEpoch: 0321 train_loss= 0.27630 time= 0.00469\nEpoch: 0322 train_loss= 0.27592 time= 0.00475\nEpoch: 0323 train_loss= 0.27613 time= 0.00337\nEpoch: 0324 train_loss= 0.27673 time= 0.00416\nEpoch: 0325 train_loss= 0.27737 time= 0.00475\nEpoch: 0326 train_loss= 0.28061 time= 0.00461\nEpoch: 0327 train_loss= 0.28164 time= 0.00359\nEpoch: 0328 train_loss= 0.28653 time= 0.00367\nEpoch: 0329 train_loss= 0.27969 time= 0.00382\nEpoch: 0330 train_loss= 0.27607 time= 0.00369\nEpoch: 0331 train_loss= 0.27432 time= 0.00351\nEpoch: 0332 train_loss= 0.27536 time= 0.00448\nEpoch: 0333 train_loss= 0.27811 time= 0.00423\nEpoch: 0334 train_loss= 0.27774 time= 0.00330\nEpoch: 0335 train_loss= 0.28031 time= 0.00438\nEpoch: 0336 train_loss= 0.27662 time= 0.00439\nEpoch: 0337 train_loss= 0.27364 time= 0.00381\nEpoch: 0338 train_loss= 0.27280 time= 0.00337\nEpoch: 0339 train_loss= 0.27376 time= 0.00330\nEpoch: 0340 train_loss= 0.27649 time= 0.00419\nEpoch: 0341 train_loss= 0.27795 time= 0.00322\nEpoch: 0342 train_loss= 0.28578 time= 0.00437\nEpoch: 0343 train_loss= 0.27934 time= 0.00302\nEpoch: 0344 train_loss= 0.27428 time= 0.00389\nEpoch: 0345 train_loss= 0.27175 time= 0.00324\nEpoch: 0346 train_loss= 0.27341 time= 0.00320\nEpoch: 0347 train_loss= 0.27595 time= 0.00328\nEpoch: 0348 train_loss= 0.27623 time= 0.00391\nEpoch: 0349 train_loss= 0.28273 time= 0.00300\nEpoch: 0350 train_loss= 0.27456 time= 0.00282\nEpoch: 0351 train_loss= 0.27308 time= 0.00302\nEpoch: 0352 train_loss= 0.27858 time= 0.00396\nEpoch: 0353 train_loss= 0.27464 time= 0.00343\nEpoch: 0354 train_loss= 0.27456 time= 0.00377\nEpoch: 0355 train_loss= 0.27211 time= 0.00420\nEpoch: 0356 train_loss= 0.27096 time= 0.00315\nEpoch: 0357 train_loss= 0.27311 time= 0.00318\nEpoch: 0358 train_loss= 0.27457 time= 0.00375\nEpoch: 0359 train_loss= 0.27150 time= 0.00390\nEpoch: 0360 train_loss= 0.27382 time= 0.00384\nEpoch: 0361 train_loss= 0.27559 time= 0.00337\nEpoch: 0362 train_loss= 0.27444 time= 0.00373\nEpoch: 0363 train_loss= 0.27495 time= 0.00300\nEpoch: 0364 train_loss= 0.28717 time= 0.00294\nEpoch: 0365 train_loss= 0.28678 time= 0.00278\nEpoch: 0366 train_loss= 0.28219 time= 0.00427\nEpoch: 0367 train_loss= 0.29330 time= 0.00400\nEpoch: 0368 train_loss= 0.28322 time= 0.00387\nEpoch: 0369 train_loss= 0.30885 time= 0.00296\nEpoch: 0370 train_loss= 0.31057 time= 0.00363\nEpoch: 0371 train_loss= 0.29231 time= 0.00291\nEpoch: 0372 train_loss= 0.31176 time= 0.00338\nEpoch: 0373 train_loss= 0.33323 time= 0.00303\nEpoch: 0374 train_loss= 0.32320 time= 0.00320\nEpoch: 0375 train_loss= 0.30973 time= 0.00338\nEpoch: 0376 train_loss= 0.42351 time= 0.00430\nEpoch: 0377 train_loss= 0.45059 time= 0.00398\nEpoch: 0378 train_loss= 0.37679 time= 0.00412\nEpoch: 0379 train_loss= 0.35286 time= 0.00386\nEpoch: 0380 train_loss= 0.32546 time= 0.00390\nEpoch: 0381 train_loss= 0.34465 time= 0.00419\nEpoch: 0382 train_loss= 0.31695 time= 0.00383\nEpoch: 0383 train_loss= 0.33673 time= 0.00396\nEpoch: 0384 train_loss= 0.29745 time= 0.00355\nEpoch: 0385 train_loss= 0.32426 time= 0.00461\nEpoch: 0386 train_loss= 0.30470 time= 0.00459\nEpoch: 0387 train_loss= 0.29619 time= 0.00434\nEpoch: 0388 train_loss= 0.30654 time= 0.00294\nEpoch: 0389 train_loss= 0.29640 time= 0.00306\nEpoch: 0390 train_loss= 0.29491 time= 0.00385\nEpoch: 0391 train_loss= 0.29686 time= 0.00472\nEpoch: 0392 train_loss= 0.28895 time= 0.00435\nEpoch: 0393 train_loss= 0.28300 time= 0.00390\nEpoch: 0394 train_loss= 0.28780 time= 0.00420\nEpoch: 0395 train_loss= 0.28841 time= 0.00404\nEpoch: 0396 train_loss= 0.28461 time= 0.00371\nEpoch: 0397 train_loss= 0.28718 time= 0.00299\nEpoch: 0398 train_loss= 0.27832 time= 0.00395\nEpoch: 0399 train_loss= 0.27788 time= 0.00425\nEpoch: 0400 train_loss= 0.27976 time= 0.00419\nEpoch: 0401 train_loss= 0.27819 time= 0.00438\nEpoch: 0402 train_loss= 0.27803 time= 0.00310\nEpoch: 0403 train_loss= 0.27690 time= 0.00429\nEpoch: 0404 train_loss= 0.27503 time= 0.00323\nEpoch: 0405 train_loss= 0.27404 time= 0.00381\nEpoch: 0406 train_loss= 0.27421 time= 0.00424\nEpoch: 0407 train_loss= 0.27221 time= 0.00429\nEpoch: 0408 train_loss= 0.27288 time= 0.00339\nEpoch: 0409 train_loss= 0.27180 time= 0.00386\nEpoch: 0410 train_loss= 0.27126 time= 0.00323\nEpoch: 0411 train_loss= 0.27089 time= 0.00315\nEpoch: 0412 train_loss= 0.26978 time= 0.00303\nEpoch: 0413 train_loss= 0.26980 time= 0.00389\nEpoch: 0414 train_loss= 0.26922 time= 0.00342\nEpoch: 0415 train_loss= 0.26831 time= 0.00355\nEpoch: 0416 train_loss= 0.26803 time= 0.00411\nEpoch: 0417 train_loss= 0.26765 time= 0.00333\nEpoch: 0418 train_loss= 0.26691 time= 0.00328\nEpoch: 0419 train_loss= 0.26714 time= 0.00412\nEpoch: 0420 train_loss= 0.26623 time= 0.00323\nEpoch: 0421 train_loss= 0.26614 time= 0.00389\nEpoch: 0422 train_loss= 0.26560 time= 0.00350\nEpoch: 0423 train_loss= 0.26519 time= 0.00340\nEpoch: 0424 train_loss= 0.26511 time= 0.00378\nEpoch: 0425 train_loss= 0.26471 time= 0.00413\nEpoch: 0426 train_loss= 0.26441 time= 0.00285\nEpoch: 0427 train_loss= 0.26403 time= 0.00422\nEpoch: 0428 train_loss= 0.26365 time= 0.00313\nEpoch: 0429 train_loss= 0.26352 time= 0.00418\nEpoch: 0430 train_loss= 0.26324 time= 0.00368\nEpoch: 0431 train_loss= 0.26312 time= 0.00369\nEpoch: 0432 train_loss= 0.26272 time= 0.00414\nEpoch: 0433 train_loss= 0.26250 time= 0.00295\nEpoch: 0434 train_loss= 0.26225 time= 0.00434\nEpoch: 0435 train_loss= 0.26207 time= 0.00425\nEpoch: 0436 train_loss= 0.26187 time= 0.00448\nEpoch: 0437 train_loss= 0.26158 time= 0.00436\nEpoch: 0438 train_loss= 0.26140 time= 0.00382\nEpoch: 0439 train_loss= 0.26118 time= 0.00425\nEpoch: 0440 train_loss= 0.26103 time= 0.00292\nEpoch: 0441 train_loss= 0.26082 time= 0.00318\nEpoch: 0442 train_loss= 0.26060 time= 0.00321\nEpoch: 0443 train_loss= 0.26038 time= 0.00346\nEpoch: 0444 train_loss= 0.26022 time= 0.00282\nEpoch: 0445 train_loss= 0.26009 time= 0.00378\nEpoch: 0446 train_loss= 0.25989 time= 0.00308\nEpoch: 0447 train_loss= 0.25970 time= 0.00314\nEpoch: 0448 train_loss= 0.25952 time= 0.00471\nEpoch: 0449 train_loss= 0.25939 time= 0.00333\nEpoch: 0450 train_loss= 0.25920 time= 0.00317\nEpoch: 0451 train_loss= 0.25903 time= 0.00377\nEpoch: 0452 train_loss= 0.25887 time= 0.00299\nEpoch: 0453 train_loss= 0.25872 time= 0.00314\nEpoch: 0454 train_loss= 0.25856 time= 0.00293\nEpoch: 0455 train_loss= 0.25841 time= 0.00325\nEpoch: 0456 train_loss= 0.25827 time= 0.00409\nEpoch: 0457 train_loss= 0.25810 time= 0.00307\nEpoch: 0458 train_loss= 0.25795 time= 0.00318\nEpoch: 0459 train_loss= 0.25780 time= 0.00382\nEpoch: 0460 train_loss= 0.25765 time= 0.00339\nEpoch: 0461 train_loss= 0.25750 time= 0.00414\nEpoch: 0462 train_loss= 0.25735 time= 0.00380\nEpoch: 0463 train_loss= 0.25721 time= 0.00392\nEpoch: 0464 train_loss= 0.25706 time= 0.00413\nEpoch: 0465 train_loss= 0.25692 time= 0.00371\nEpoch: 0466 train_loss= 0.25678 time= 0.00423\nEpoch: 0467 train_loss= 0.25664 time= 0.00369\nEpoch: 0468 train_loss= 0.25650 time= 0.00412\nEpoch: 0469 train_loss= 0.25635 time= 0.00402\nEpoch: 0470 train_loss= 0.25622 time= 0.00425\nEpoch: 0471 train_loss= 0.25608 time= 0.00318\nEpoch: 0472 train_loss= 0.25594 time= 0.00311\nEpoch: 0473 train_loss= 0.25580 time= 0.00398\nEpoch: 0474 train_loss= 0.25566 time= 0.00294\nEpoch: 0475 train_loss= 0.25553 time= 0.00358\nEpoch: 0476 train_loss= 0.25540 time= 0.00310\nEpoch: 0477 train_loss= 0.25526 time= 0.00314\nEpoch: 0478 train_loss= 0.25513 time= 0.00357\nEpoch: 0479 train_loss= 0.25499 time= 0.00376\nEpoch: 0480 train_loss= 0.25486 time= 0.00408\nEpoch: 0481 train_loss= 0.25473 time= 0.00300\nEpoch: 0482 train_loss= 0.25459 time= 0.00411\nEpoch: 0483 train_loss= 0.25446 time= 0.00388\nEpoch: 0484 train_loss= 0.25433 time= 0.00293\nEpoch: 0485 train_loss= 0.25420 time= 0.00296\nEpoch: 0486 train_loss= 0.25407 time= 0.00379\nEpoch: 0487 train_loss= 0.25394 time= 0.00347\nEpoch: 0488 train_loss= 0.25381 time= 0.00287\nEpoch: 0489 train_loss= 0.25368 time= 0.00413\nEpoch: 0490 train_loss= 0.25355 time= 0.00435\nEpoch: 0491 train_loss= 0.25342 time= 0.00395\nEpoch: 0492 train_loss= 0.25329 time= 0.00403\nEpoch: 0493 train_loss= 0.25316 time= 0.00326\nEpoch: 0494 train_loss= 0.25303 time= 0.00409\nEpoch: 0495 train_loss= 0.25291 time= 0.00363\nEpoch: 0496 train_loss= 0.25278 time= 0.00297\nEpoch: 0497 train_loss= 0.25265 time= 0.00293\nEpoch: 0498 train_loss= 0.25253 time= 0.00452\nEpoch: 0499 train_loss= 0.25240 time= 0.00299\nEpoch: 0500 train_loss= 0.25227 time= 0.00390\nOptimization Finished!\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"emb.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rNoyuRwXmkWv","outputId":"b596a042-d1a2-4bd1-a565-a157fac2bded","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:15:22.897335Z","iopub.execute_input":"2025-03-15T17:15:22.897654Z","iopub.status.idle":"2025-03-15T17:15:22.902526Z","shell.execute_reply.started":"2025-03-15T17:15:22.897622Z","shell.execute_reply":"2025-03-15T17:15:22.901681Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"(438, 16)"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"pd.DataFrame(emb).to_csv(\"GAE-128-8-new.csv\")","metadata":{"id":"brRRfmpVo_P2","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:15:44.406396Z","iopub.execute_input":"2025-03-15T17:15:44.406691Z","iopub.status.idle":"2025-03-15T17:15:44.418059Z","shell.execute_reply.started":"2025-03-15T17:15:44.406667Z","shell.execute_reply":"2025-03-15T17:15:44.417154Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}